---
title: "hw-03"
author: "Siddhant Pujni (S2344216)"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
## **DO NOT EDIT THIS CODE CHUNK**view
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
```


## Data Load

```{r read_data}
gss16<-read.csv("data/gss16.csv")
```

## Data preparation before modelling

The `gss16` data set needs to be prepared prior to modelling. This is not an assessed component to your assignment, but it is an important part of the data science process. Carefully copy the following code into the beginning of your `.Rmd` file and ensure that you understand how the data has been prepared.

### Cleaning and selecting rows 

Create a new data frame called `gss16_advfront` that includes the variables `advfront`, `emailhr` (Number of hours spent on email weekly), `educ` (education level), `polviews` (political views) and `wrkstat` (working status). Remove any row that contains any `NA`s using the `drop_na()` command

```{r}
gss16_advfront <- gss16 %>%
  select(advfront, emailhr, educ, polviews, wrkstat) %>%
  drop_na()
```

### Re-levelling `advfront`

The `advfront` variable contains responses to the question "Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government." The possible answers are on the 5-point Likert scale: `"Strongly agree"`, `"Agree"`, `"Dont know"`, `"Disagree"` and `"Strongly disagree"`. For the purpose of this assignment, the `advfront` variable needs to be transformed such that it has two levels: 

* `"Agree"` - combining the options `"Strongly agree"` and `"Agree"`.
* `"Not agree"` - combining the options `"Dont know"`, `"Disagree"` and `"Strongly disagree"`.

The following code does that this re-levelling:

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    advfront = case_when(
      advfront == "Strongly agree" ~ "Agree",
      advfront == "Agree" ~ "Agree",
      TRUE ~ "Not agree"
    ),
    advfront = fct_relevel(advfront, "Not agree", "Agree")
  )
```


### Re-levelling `polviews`


```{marginfigure}
You can do this in various ways. One option is to use the `str_detect()` function to detect the existence of words like liberal or conservative. Note that these sometimes show up with lowercase first letters and sometimes with upper case first letters. To detect either in the `str_detect()` function, you can use "[Ll]iberal" and "[Cc]onservative". But feel free to solve the problem however you like, this is just one option!
```

The `polviews` variable contains information about the participant's political position within 7 categories ranging from `"Extremely liberal"` to `"Extrmly conservative"`. Here we wish to simplify the range of options to 3 categories - `"Conservative"` , `"Moderate"`, and `"Liberal"`. This is achieved via the following code:

```{r}
gss16_advfront <- gss16_advfront %>%
  mutate(
    polviews = case_when(
      str_detect(polviews, "[Cc]onservative") ~ "Conservative",
      str_detect(polviews, "[Ll]iberal") ~ "Liberal",
      TRUE ~ polviews
    ),
    polviews = fct_relevel(polviews, "Conservative", "Moderate", "Liberal")
  )
```

Please see [chapter 14 of R for Data Science](https://r4ds.had.co.nz/strings.html) for more on string processing commands.

You now have the cleaned and pre-processed data to use for modelling, you can continue with the `gss16_advfront` for the following questions. 

## Exercise 1: Create a linear regression model

Consider the numerical values `educ` and `emailhr` from your new data set `gss16_advfront`.

a) Fit a linear regression model to predicting `emailhr` based on the `educ`. From your output, state the formula for the line-of-best fit and give an interpretation of the `emailhr` estimate.

b) Comment on the overall performance of the linear regression model. Support your statements with an appropriate data visualisation and model fit statistics.

```{r}

linear_reg() %>%
  set_engine("lm") %>%
  fit(emailhr ~ educ, data = gss16_advfront) %>%
  tidy()

cat("The formula for the line of best fit if y = emailhr and x = educ is given by: y = 0.69x - 2.76", "\n")

gss16_advfront %>%
  ggplot(mapping = aes(x = educ, y = emailhr)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, color = "orange") +
  labs(
    title = "Linear Regression model",
    subtitle = "Number of years in education vs. Number of hours spent on email per week",
    x = "Number of years in education",
    y = "Number of hours spent on email per week"
    )+
  theme_minimal()

use glance

```

*Exercise 1:*

a) The y-intercept of the line of best fit implies that if the individual has no education then they spend -2.76 hours checking their email on a weekly basis. This does not make sense as it is not possible to spend 'negative' hours checking your email, the lowest value should be 0. This implies that a linear fit may not be the best model for this set of data. Moreover, the slope implies that on average, for each additional year spent in education, the amount of time people spend checking their email each week is expected to be higher, on average, by 0.69 hours.  

b) Overall, the model does not seem to be an ideal fit because the y-intercept is negative which does not make physical sense. Moreover, looking at the scatter plot with a linear model on it the data does not appear to have a linear relationship at all as the same number of years in education have a range of unique values for the number of hours being spent on their email, suggesting there is not really any relationship between these two variables. This is further supported by the statistics...


## Exercise 2: Create a workflow to fit a model

In this part, we're going to build a model to predict whether someone agrees or doesn't agree with the following statement:

Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government.

The responses to the question on the GSS about this statement are in the `advfront` variable, in the `gss16_advfront` data that you obtained.

First, use the following code to split the dataset into a training dataset (`gss16_train`) and a testing dataset (`gss16_test`). This code splits the data into 75\% training and 25\% testing.

```{r split-data}
set.seed(1234)
gss16_split <- initial_split(gss16_advfront)
gss16_train <- training(gss16_split)
gss16_test  <- testing(gss16_split)
```

a) Build a workflow for the training data that consists of a recipe (`gss16_rec_1`) and a model (`gss16_mod_1`). Name this workflow `gss16_wflow_1`.
    
The recipe (named `gss16_rec_1`) should contain the following steps for predicting `advfront` from `educ`:

  - `step_dummy()` to create dummy variables for `all_nominal()` variables that are predictors, i.e. not "outcomes". You can select outcomes using `all_outcomes()`

  - The model (named `gss16_mod_1`) should specify a model that is appropriate for the data as well as the computational engine.

```{r}

set.seed(1234)

gss16_rec_1 <- recipe(
  advfront ~ educ, 
  data = gss16_train
  ) %>%
  step_dummy(all_nominal(), -all_outcomes())

# Defining the model
gss16_mod_1 <- logistic_reg() %>%
  set_engine("glm")

# Building the workflow
gss16_wflow_1 <- workflow() %>%
  add_recipe(gss16_rec_1) %>%
  add_model(gss16_mod_1)

# Train the model using the training data
gss16_wflow_1 %>%
  fit(data = gss16_train) %>%
  tidy()

```

b) Explain why you have chosen the model that you have selected.

I chose a logistic regression because we were looking at a binary outcome model/prediction where they either agree or disagree with the statement given, hence the data wrangling we did earlier on in the code.

## Exercise 3: Logistic regression with single predictor

a) Apply the workflow you defined earlier to the training dataset and named the model as `gss16_fit_1`. Display the resulting tibble containing the fitted model parameters. 

```{r}

```

b) Use the fitted models to predict the test data, plot the ROC curves for the predictions.

```{r}

```

## Exercise 4: Logistic regression modelling and interpretation

We are now going to model `advfront` using the explanatory variables `polviews`, `wrkstat`, and `educ`.

a) Build a new workflow for the training data that consists of a recipe (`gss16_rec_2`) and a model (`gss16_mod_2`). Name this workflow `gss16_wflow_2`. You can simply **copy, paste and edit** the code from earlier.
    
Now the new recipe (named `gss16_rec_2`) should contain the followings for predicting `advfront` from `polviews`, `wrkstat`, and `educ`:

  - `step_dummy()` to create dummy variables for `all_nominal()` variables that are predictors, i.e. not "outcomes". You can select outcomes using `all_outcomes()`

  - The model (named `gss16_mod_2`) should specify a model that is appropriate for the data as well as the computational engine.
  
Apply the new workflow to the training dataset and create a new model fit 
named as `gss16_fit_2`. 

Then use the fitted models to predict the test data, plot the ROC curve for the predictions for both models, and calculate the areas under the ROC curves.

```{r}

```

b) Comment on which model performs better 

  * the model only including `educ`, as model 1 (`gss16_fit_1`) 
  * the model including `polviews`, `wrkstat`, and `educ` with `gss16_split` as model 2 (`gss16_fit_2`)
  
Explain your reasoning.

```{r}

```



